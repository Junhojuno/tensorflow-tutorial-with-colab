{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2.cnn_mnist_tensorflow_colab.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Junhojuno/tensorflow-tutorial-with-colab/blob/master/2_cnn_mnist_tensorflow_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "xTG8mdNkac8S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "outputId": "436db2f9-51bd-4f42-b570-595532041ce9"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets('data/', one_hot=True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-2-d113291f1e43>:4: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use urllib or similar directly.\n",
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting data/train-images-idx3-ubyte.gz\n",
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting data/train-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Extracting data/t10k-images-idx3-ubyte.gz\n",
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NmBSIoJDZoMy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# data loading\n",
        "train_images = mnist.train.images\n",
        "train_labels = mnist.train.labels\n",
        "test_images = mnist.test.images\n",
        "test_labels = mnist.test.labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G8MoDRzHbf6S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f4222052-e665-48f0-e03f-b08b19b10399"
      },
      "cell_type": "code",
      "source": [
        "n_input = 784\n",
        "n_channel = 64 # filter 갯수\n",
        "n_classes = 10\n",
        "\n",
        "x = tf.placeholder(dtype='float', shape=[None, n_input])\n",
        "y = tf.placeholder(dtype='float', shape=[None, n_classes])\n",
        "\n",
        "# setting parameters\n",
        "# filter size는 3x3이고 data가 흑백이므로 3x3x1 filter weights \n",
        "weights = {\n",
        "    'convolution1' : tf.Variable(initial_value=tf.random_normal(shape=[3,3,1,n_channel], stddev=0.1)),\n",
        "    'convolution2' : tf.Variable(initial_value=tf.random_normal(shape=[3,3,n_channel,128], stddev=0.1)),\n",
        "    'dense' : tf.Variable(initial_value=tf.random_normal(shape=[7*7*128,n_classes], stddev=0.1))\n",
        "}\n",
        "\n",
        "biases = {\n",
        "    'convolution1' : tf.Variable(initial_value=tf.random_normal(shape=[n_channel], stddev=0.1)),\n",
        "    'convolution2' : tf.Variable(initial_value=tf.random_normal(shape=[128], stddev=0.1)),\n",
        "    'dense' : tf.Variable(initial_value=tf.random_normal(shape=[n_classes], stddev=0.1))\n",
        "}\n",
        "\n",
        "print(\"parameters setting...!\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "parameters setting...!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "n1OgWec8d2w6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# network setting\n",
        "def CNN(x,weights, biases):\n",
        "  \n",
        "  # 1st layer\n",
        "  # input reshape\n",
        "  x = tf.reshape(x,shape=[-1,28,28,1])\n",
        "  # convolution! + bias\n",
        "  conv1 = tf.nn.conv2d(x, filter=weights['convolution1'], strides=[1,1,1,1], padding='SAME')\n",
        "  conv1 = tf.nn.bias_add(conv1, biases['convolution1'])\n",
        "  # activation function\n",
        "  conv1 = tf.nn.relu(conv1)\n",
        "  # max pooling with non-overlapping\n",
        "  conv1 = tf.nn.max_pool(conv1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
        "  \n",
        "  # 2nd layer\n",
        "  conv2 = tf.nn.conv2d(conv1, filter=weights['convolution2'], strides=[1,1,1,1], padding='SAME')\n",
        "  conv2 = tf.nn.bias_add(conv2, bias=biases['convolution2'])\n",
        "  conv2 = tf.nn.relu(conv2)\n",
        "  conv2 = tf.nn.max_pool(conv2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
        "  \n",
        "  # fully connected layer(dense layer)\n",
        "  # 한줄로 펴기 -1 : 데이터 갯수\n",
        "  dense = tf.reshape(conv2, shape=[-1,weights['dense'].get_shape().as_list()[0]])\n",
        "  logit = tf.add(tf.matmul(dense,weights['dense']), biases['dense'])\n",
        "  \n",
        "  return logit\n",
        "\n",
        "\n",
        "cnn_output = CNN(x, weights, biases)\n",
        "\n",
        "# cost(loss) & optimizer &accuracy setting\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=cnn_output))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)\n",
        "correct = tf.equal(tf.argmax(cnn_output,1), tf.argmax(y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct,'float'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2aGh7puoijBC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "outputId": "03066551-5335-4e4b-c5fb-c38249fd7b91"
      },
      "cell_type": "code",
      "source": [
        "training_epochs = 20\n",
        "batch_size = 100\n",
        "display_step = 4\n",
        "\n",
        "# Launch the Graph\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "# Optimizer\n",
        "for epoch in range(training_epochs):\n",
        "    avg_cost = 0\n",
        "    total_batch = int(mnist.train.num_examples / batch_size)\n",
        "    # Iteration\n",
        "    for i in range(total_batch):\n",
        "        batch_xs, batch_ys = mnist.train.next_batch(batch_size=batch_size)\n",
        "        feeds = {x: batch_xs, y: batch_ys}\n",
        "        sess.run(optimizer, feed_dict=feeds)\n",
        "        avg_cost += sess.run(cost, feed_dict=feeds)\n",
        "    avg_cost = avg_cost / total_batch\n",
        "    # Display\n",
        "    if (epoch + 1) % display_step == 0:\n",
        "        print(\"Epoch: %03d/%03d cost: %.9f\" % (epoch+1, training_epochs, avg_cost))\n",
        "        feeds = {x: batch_xs, y: batch_ys}\n",
        "        train_acc = sess.run(accuracy, feed_dict=feeds)\n",
        "        print(\"Train Accuracy: %.3f\" % (train_acc))\n",
        "        feeds = {x: test_images[:100, :], y: test_labels[:100, :]}\n",
        "        test_acc = sess.run(accuracy, feed_dict=feeds)\n",
        "        print(\"Test Accuracy: %.3f\" % (test_acc))\n",
        "    \n",
        "print(\"Optimization Finished!!!\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 004/020 cost: 0.027440274\n",
            "Train Accuracy: 0.980\n",
            "Test Accuracy: 0.990\n",
            "Epoch: 008/020 cost: 0.009395549\n",
            "Train Accuracy: 1.000\n",
            "Test Accuracy: 0.980\n",
            "Epoch: 012/020 cost: 0.003462189\n",
            "Train Accuracy: 1.000\n",
            "Test Accuracy: 0.980\n",
            "Epoch: 016/020 cost: 0.001500432\n",
            "Train Accuracy: 1.000\n",
            "Test Accuracy: 0.980\n",
            "Epoch: 020/020 cost: 0.001563977\n",
            "Train Accuracy: 1.000\n",
            "Test Accuracy: 0.980\n",
            "Optimization Finished!!!\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}